[["index.html", "Creating the LCDmix R package 1 Preliminaries 1.1 DESCRIPTION file 1.2 Package-level documentation", " Creating the LCDmix R package Sang-wook Lee 2025-06-23 1 Preliminaries This document uses litr to define the LCDmix R package. #remotes::install_github(&quot;jacobbien/litr-project@*release&quot;, subdir = &quot;litr&quot;) litr::render(&quot;index.Rmd&quot;) 1.1 DESCRIPTION file usethis::create_package( path = &quot;.&quot;, fields = list( Package = params$package_name, Version = &quot;0.0.0.9000&quot;, Title = &quot;Mixture of log-concave regressions&quot;, Description = &quot;This package uses mixture of log-concave regressions to analyze complex and heterogenous data.&quot;, `Authors@R` = person( given = &quot;Sang-wook&quot;, family = &quot;Lee&quot;, email = &quot;sangwook@usc.edu&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;) ) ) ) usethis::use_mit_license(copyright_holder = &quot;F. Last&quot;) 1.2 Package-level documentation #&#39; Mixture of log-concave regressions modeling #&#39; #&#39; This package uses mixture of log-concave regressions to analyze complex and heterogenous data. #&#39; #&#39; @docType package "],["the-model.html", "2 The model", " 2 The model Let \\(X^{(t)} \\in \\mathbb{R}^{p}\\) be given fixed design points and \\(Y_i^{(t)} \\in \\mathbb{R}^{d}\\) be the observed dependent variables, for \\(1 \\le i \\le n_t, \\ 1 \\le t \\le T\\). We model \\(Y_i^{(t)}\\) as i.i.d. draws from \\(K\\) different \\(d-\\)dimensional log-concave distributions, conditioning on \\(X^{(t)}\\). Let \\(Z_i^{(t)}\\) denote the (latent) cluster membership so that \\(P(Z_i^{(t)} =k|X^{(t)} ) = \\pi_k(X^{(t)} )\\), for \\(1 \\le k \\le K\\). Let \\(\\mu_k:\\mathbb{R}^{p+1} \\rightarrow \\mathbb{R}^{d}\\) be the unknown regression function, for \\(1 \\le k \\le K\\) and \\(\\mu_k\\) is assumed to belong to a given family \\(M\\), which is closed under scalar addition. We further assume that \\(M\\) is the set of all affine functions so that \\(\\mu_k \\in M\\) if and only if \\(\\mu_k (x) = \\theta_{k0} + \\theta_k^T x\\) for some coefficients \\(\\theta_{k0} \\in \\mathbb{R}^{d}\\) and \\(\\theta_k \\in \\mathbb{R}^{p \\times d}\\). The conditional error \\(\\varepsilon_i^{(t)} |Z_i^{(t)} =k\\) follows a mean zero, \\(d-\\)dimensional log-concave density \\(exp(g_k)\\) so that \\(Y_i^{(t)} = \\theta_{k0} + \\theta_k^T X^{(t)} + \\varepsilon_i^{(t)}, \\text{ if } Z_i^{(t)} =k\\). Then, \\[ Y_i^{(t)} |Z_i^{(t)} = k, X^{(t)} \\ \\sim \\ exp[g_k(\\cdot - \\theta_{k0} - \\theta_k^T X^{(t)} )] \\] so that \\[ Y_i^{(t)} | X^{(t)} \\ \\sim \\ \\sum_{k=1}^K exp[g_k(\\cdot - \\theta_{k0} - \\theta_k^T X^{(t)} )]\\pi_k(X^{(t)} ) \\] We model \\[ \\pi_{tk}(\\alpha ) = \\pi_k(X^{(t)} ; \\alpha) = \\frac{exp(\\alpha_{k0} + \\alpha_k^T X^{(t)} )}{\\sum_{l=1}^K exp(\\alpha_{l0} + \\alpha_l^T X^{(t)} )} \\] where \\(\\alpha = \\{ \\alpha_{k0}, \\alpha_{k} \\}_{k=1}^K\\) is a collection of coefficients \\(\\alpha_{k0} \\in \\mathbb{R}\\) and \\(\\alpha_k \\in \\mathbb{R}^{p}\\) for \\(1 \\le k \\le K\\). What we want to maximize here is the mixture of log-likelihood: \\[\\begin{align*} L(\\alpha, \\theta, g; X, Y) &amp;= \\frac{1}{N} \\sum_{t=1}^T \\sum_{i=1}^{n_t} \\log \\mathbb{P}(Y_i^{(t)} |X^{(t)} ) \\\\ &amp;= \\frac{1}{N} \\sum_{t=1}^T \\sum_{i=1}^{n_t} \\log \\left[ \\sum_{k=1}^K exp \\left( g_k \\left(Y_i^{(t)} -\\theta_{k0} - \\theta_k^T X^{(t)} \\right) \\right) \\cdot \\pi_{tk}(\\alpha) \\right] \\end{align*}\\] where the parameters are \\(\\alpha,\\ \\theta = \\{ \\theta_{k0},\\theta_k \\}_{k=1}^K,\\ g = \\{ g_k \\}_{k=1}^K\\) where \\(g_k\\) is a logdensity of a logconcave density \\(exp(g_k)\\) for \\(1 \\le k \\le K\\), and \\(X = \\{X^{(t)} \\}_{t=1}^T,\\ Y = \\{ Y_i^{(t)} \\}_{i = 1,..., n_t}^{t = 1,..., T},\\ N = \\sum_{t=1}^T n_t\\) But directly optimizing \\(L(\\alpha, \\theta, g; X, Y)\\) is difficult due to its non-convexity. Instead, using the membership \\(Z_i^{(t)}\\), we define the joint log-likelihood: \\[\\begin{align*} \\Lambda(\\alpha, \\theta, g; X, Y, Z) &amp;= \\frac{1}{N} \\sum_{t=1}^T \\sum_{i=1}^{n_t} \\sum_{k=1}^K \\mathbb{I}\\{Z_i^{(t)} =k\\} \\log \\mathbb{P}(Y_i^{(t)} , Z_i^{(t)} = k |X^{(t)} ) \\\\ &amp;= \\frac{1}{N} \\sum_{t, i,k} \\mathbb{I}\\{Z_i^{(t)} =k\\} \\left[ g_k \\left(Y_i^{(t)} -\\theta_{k0} -\\theta_k^T X^{(t)} \\right) + \\log \\pi_{tk}(\\alpha) \\right] \\end{align*}\\] Since we can’t observe \\(Z_i^{(t)}\\), we define the surrogate function, which is the conditional expectation of \\(\\Lambda(\\alpha, g, \\theta; X, Y, Z)\\) with respect to the membership \\(Z_i^{(t)}\\), conditioning on \\(X^{(t)}\\) and \\(Y_i^{(t)}\\): \\[ Q(\\alpha, g, \\theta) = Q(\\alpha, g, \\theta; X, Y) = \\frac{1}{N} \\sum_{t=1}^T \\sum_{i=1}^{n_t} \\sum_{k=1}^K r_{tik} \\left[ g_k \\left(Y_i^{(t)} -\\theta_{k0} -\\theta_k^T X^{(t)} \\right)) + \\log \\pi_{tk}(\\alpha) \\right] \\] where the responsibility \\(r_{tik}\\) is defined \\[\\begin{align*} r_{tik} = r_{tik}(\\alpha, \\theta, g) = P(Z_i^{(t)} = k |Y_i^{(t)} , X^{(t)} ; \\alpha, \\theta, g) &amp;= \\frac{exp(g_k(Y_i^{(t)} -\\theta_{k0} -\\theta_k^T X^{(t)} )) \\pi_{tk}(\\alpha)} {\\sum_{l=1}^K exp(g_l(Y_i^{(t)} -\\theta_{l0} -\\theta_l^T X^{(t)} )) \\pi_{tl}(\\alpha) } \\end{align*}\\] Note that the responsibility is a probability, so it should sum up to one with respect to k in the sense that \\[ \\sum_{k=1}^K r_{tik} = \\sum_{k=1}^K P(Z_i^{(t)} = k |Y_i^{(t)} , X^{(t)} ) = 1 \\] It will be useful to have data generated from this model for testing purposes, so we begin by defining a function for simulating from this model. We’ve used some functions from other packages, so let’s include those in our package: usethis::use_pipe() ## ✔ Adding magrittr to &#39;Imports&#39; field in DESCRIPTION. ## ✔ Writing &#39;R/utils-pipe.R&#39;. ## ☐ Run `devtools::document()` to update &#39;NAMESPACE&#39;. usethis::use_package(&quot;flowmix&quot;) ## Registered S3 methods overwritten by &#39;RcppEigen&#39;: ## method from ## predict.fastLm RcppArmadillo ## print.fastLm RcppArmadillo ## summary.fastLm RcppArmadillo ## print.summary.fastLm RcppArmadillo ## Warning: replacing previous import &#39;RcppArmadillo::fastLmPure&#39; by ## &#39;RcppEigen::fastLmPure&#39; when loading &#39;flowmix&#39; ## Warning: replacing previous import &#39;RcppArmadillo::fastLm&#39; by ## &#39;RcppEigen::fastLm&#39; when loading &#39;flowmix&#39; ## ✔ Adding flowmix to &#39;Imports&#39; field in DESCRIPTION. ## ☐ Refer to functions with `flowmix::fun()`. usethis::use_package(&quot;logcondens&quot;) ## ✔ Adding logcondens to &#39;Imports&#39; field in DESCRIPTION. ## ☐ Refer to functions with `logcondens::fun()`. usethis::use_package(&quot;glmnet&quot;) ## ✔ Adding glmnet to &#39;Imports&#39; field in DESCRIPTION. ## ☐ Refer to functions with `glmnet::fun()`. usethis::use_package(&quot;Matrix&quot;) ## ✔ Adding Matrix to &#39;Imports&#39; field in DESCRIPTION. ## ☐ Refer to functions with `Matrix::fun()`. usethis::use_package(&quot;lpSolve&quot;) ## ✔ Adding lpSolve to &#39;Imports&#39; field in DESCRIPTION. ## ☐ Refer to functions with `lpSolve::fun()`. usethis::use_package(&quot;Rsymphony&quot;) ## ✔ Adding Rsymphony to &#39;Imports&#39; field in DESCRIPTION. ## ☐ Refer to functions with `Rsymphony::fun()`. usethis::use_package(&quot;ggplot2&quot;) ## ✔ Adding ggplot2 to &#39;Imports&#39; field in DESCRIPTION. ## ☐ Refer to functions with `ggplot2::fun()`. usethis::use_package(&quot;parallel&quot;) ## ✔ Adding parallel to &#39;Imports&#39; field in DESCRIPTION. ## ☐ Refer to functions with `parallel::fun()`. usethis::use_package(&quot;assertthat&quot;) ## ✔ Adding assertthat to &#39;Imports&#39; field in DESCRIPTION. ## ☐ Refer to functions with `assertthat::fun()`. usethis::use_package(&quot;stats&quot;) ## ✔ Adding stats to &#39;Imports&#39; field in DESCRIPTION. ## ☐ Refer to functions with `stats::fun()`. usethis::use_package(&quot;sn&quot;) ## ✔ Adding sn to &#39;Imports&#39; field in DESCRIPTION. ## ☐ Refer to functions with `sn::fun()`. "],["the-method.html", "3 The method", " 3 The method We are using Expectation-Maximization(EM) algorithm to maximize the surrogate log-likelihood \\(Q(\\alpha, g, \\theta) = \\frac{1}{N} \\sum_{t=1}^T \\sum_{i=1}^{n_t} \\sum_{k=1}^K r_{tik} \\left[ g_k \\left(Y_i^{(t)} -\\theta_{k0} -\\theta_k^T X^{(t)} \\right)) + \\log \\pi_{tk}(\\alpha) \\right]\\). Here’s a high-level look at the algorithm. #&#39; Mixture of log-concave regression #&#39; #&#39; # mixLcdReg &lt;- function(X, # Y, # K, # B = 40, # min_count_ratio = 0, # r_bar, # lambda_alpha, # lambda_theta, # max_iter = 100, # iter_eta = 1e-6) { # preprocessing # initialization with flexmix # iteration # for (i in seq(max_iter)) { ## E-step ## M-step ### M-step alpha ### M-step theta ### M-step shift ### M-step g ## termination criteria # } # return #} main = function(Y, X, biomass, binned = F, B = 40, K, lambda_alpha = 1e-3, lambda_theta = 1e-3, nrep_flowmix = 1, max_iter = 30, iter_eta = 1e-3, maxdev = NULL, r_bar = 1e-3){ # binning if (binned){ Y_bin = Y bin_mass = biomass } else { bin = binning(Y, biomass, B) Y_bin = bin$Y_bin bin_mass = bin$bin_mass } print(&#39;passed binning&#39;) # initial GMR initial = initialization(Y_bin, X, bin_mass, K, lambda_alpha, lambda_theta, nrep_flowmix, maxdev, r_bar) print(&#39;passed init&#39;) # iteration iter = iteration(Y_bin, X, bin_mass, initial, lambda_alpha, lambda_theta, iter_eta, max_iter, maxdev, r_bar) return(list(Y_bin = Y_bin, X = X, bin_mass = bin_mass, initial = initial, iter = iter)) } binning = function(Y, biomass, B = 40){ TT = length(Y) if (B == 0){ Y_bin = list() bin_mass = list() for (t in 1:TT){ tmp = tapply(biomass[[t]], factor(Y[[t]]), sum) Y_bin[[t]] = matrix(as.numeric(names(tmp)), ncol = 1) bin_mass[[t]] = as.numeric(tmp) } } else { Y_range = range(Y) bin = seq(from = Y_range[1], to = Y_range[2], length = B + 1) binnedY = lapply(Y, findInterval, bin, rightmost.closed = T) binnedY = lapply(binnedY, factor, levels = 1:B) mid = rep(0, B) for (i in 1:B){ mid[i] = (bin[i+1] + bin[i])/2 } Y_bin = list() bin_mass = list() for (t in 1:TT){ bin_mass[[t]] = tapply(biomass[[t]], binnedY[[t]], sum) # removing bins with zero counts tmp = !is.na(bin_mass[[t]]) Y_bin[[t]] = matrix(mid[tmp], ncol = 1) bin_mass[[t]] = c(bin_mass[[t]][tmp]) } names(Y_bin) = names(Y) } return(list(Y_bin = Y_bin, bin_mass = bin_mass)) } # could be improved using levels(factor(unlist(Y))) initialization = function(Y_bin, X, bin_mass, K, lambda_alpha, lambda_theta, nrep_flowmix, maxdev, r_bar){ flow = flowmix::flowmix(Y_bin, X, bin_mass, numclust = K, prob_lambda = lambda_alpha, mean_lambda = lambda_theta, nrep = nrep_flowmix, maxdev = maxdev) print(&#39;passed flowmix&#39;) ## initial alpha alpha_init = flow$alpha ## initial theta theta0_init = list() theta_init = list() for (k in 1:K){ theta0_init[[k]] = flow$beta[[k]][1] theta_init[[k]] = flow$beta[[k]][2:(ncol(X)+1)] } resi_init = calc_resi(Y_bin, X, theta0_init, theta_init) ## initial resp likeli = list() resp_init = list() weight_init = list() idx_init = list() P = pi_k(X, alpha_init) for (t in 1:length(Y_bin)){ likeli[[t]] = matrix(0, nrow = nrow(Y_bin[[t]]), ncol = K) idx_init[[t]] = matrix(F, nrow = nrow(Y_bin[[t]]), ncol = K) for (k in 1:K){ likeli[[t]][,k] = dnorm(resi_init[[t]][,k], flow$mn[t,1,k], sqrt(flow$sigma[k])) * P[t,k] } resp_init[[t]] = likeli[[t]]/rowSums(likeli[[t]]) idx_init[[t]] = resp_init[[t]] &gt; r_bar weight_init[[t]] = resp_init[[t]] * bin_mass[[t]] } ## initial theta shift theta0_init = Mstep_shift(Y_bin, X, weight_init, idx_init, theta_init) resi_init = calc_resi(Y_bin, X, theta0_init, theta_init) ## initial g g_init = Mstep_g_logcondens(resi_init, weight_init, idx_init) ## initial Q Q = calc_surr_logcondens(X, g_init, resi_init, theta_init, alpha_init, idx_init, weight_init, lambda_alpha, lambda_theta) Q_every = Q return(list(flow = flow, idx_init = idx_init, resp_init = resp_init, weight_init = weight_init, theta0_init = theta0_init, theta_init = theta_init, alpha_init = alpha_init, resi_init = resi_init, g_init = g_init, Q = Q, Q_every = Q_every)) } #&#39; calculating \\pi_{tk}(\\alpha) = P(Z_i^{(t)} = k| X^(t)) #&#39; pi_k = function(X, alpha){ p = dim(X)[2] TT = dim(X)[1] K = dim(alpha)[1] tmp = exp(t(matrix(alpha[,1], ncol = TT, nrow = K)) + X %*% t(alpha[,2:(p+1)])) pi_k = tmp / rowSums(tmp) # TT by K matrix return(pi_k) } Mstep_shift = function(Y_bin, X, weight, idx, theta){ theta0 = list() for (k in 1:length(theta)){ # except for the intercept term up = 0 down = 0 for (t in 1:length(Y_bin)){ idx_tk = idx[[t]][,k] weight_tk = weight[[t]][idx_tk,k] up = up + weight_tk %*% Y_bin[[t]][idx_tk] - sum(weight_tk) * sum(X[t,] * theta[[k]]) down = down + sum(weight_tk) } theta0[[k]] = up/down } return(theta0) } calc_resi = function(Y_bin, X, theta0, theta){ resi = list() K = length(theta0) for (t in 1:length(Y_bin)){ resi[[t]] = matrix(0, nrow = length(Y_bin[[t]]), ncol = K) for (k in 1:K){ resi[[t]][,k] = Y_bin[[t]] - c(theta0[[k]] + sum(X[t,] * theta[[k]])) } } return(resi) # length TT list, with `resi[[t]]` being N_t by K matrix } Mstep_g_logcondens = function(resi, weight, idx){ TT = length(weight) K = dim(idx[[1]])[2] g = list() for (k in 1:K){ resi_k = c() w_k = c() for (t in 1:TT){ idx_tk = idx[[t]][,k] weight_tk = weight[[t]][idx_tk,k] resi_k = c(resi_k, resi[[t]][idx_tk,k]) w_k = c(w_k, weight_tk) } uniq_resi_k = unique(resi_k) M = length(uniq_resi_k) if (M &lt; 5) { print(paste(&quot;There are only&quot;, M, &quot;points in Cluster&quot;, k)) } uniq_wk = rep(0, M) for (j in 1:M){ uniq_wk[j] = sum(w_k[uniq_resi_k[j] == resi_k]) } #g[[k]] = logcondens::activeSetLogCon(uniq_resi_k, w = uniq_wk / sum(uniq_wk), print = FALSE) g[[k]] = modified_logcondens(uniq_resi_k, w = uniq_wk / sum(uniq_wk), print = FALSE) } return(g) } modified_logcondens = function(x, xgrid = NULL, print = FALSE, w = NA){ prec &lt;- 1e-10 xn &lt;- sort(x) if ((!identical(xgrid, NULL) &amp; (!identical(w, NA)))) { stop(&quot;If w != NA then xgrid must be NULL!\\n&quot;) } if (identical(w, NA)) { tmp &lt;- logcondens::preProcess(x, xgrid = xgrid) x &lt;- tmp$x w &lt;- tmp$w sig &lt;- tmp$sig } if (!identical(w, NA)) { tmp &lt;- cbind(x, w) tmp &lt;- tmp[order(x), ] x &lt;- tmp[, 1] w &lt;- tmp[, 2] est.m &lt;- sum(w * x) est.sd &lt;- sum(w * (x - est.m)^2) est.sd &lt;- sqrt(est.sd * length(x)/(length(x) - 1)) sig &lt;- est.sd } n &lt;- length(x) phi &lt;- logcondens::LocalNormalize(x, 1:n * 0) IsKnot &lt;- 1:n * 0 IsKnot[c(1, n)] &lt;- 1 res1 &lt;- logcondens::LocalMLE(x = x, w = w, IsKnot = IsKnot, phi_o = phi, prec = prec) phi &lt;- res1$phi L &lt;- res1$L conv &lt;- res1$conv H &lt;- res1$H iter1 &lt;- 1 while ((iter1 &lt; 500) &amp; (max(H) &gt; prec * mean(abs(H)))) { IsKnot_old &lt;- IsKnot iter1 &lt;- iter1 + 1 tmp &lt;- max(H) k &lt;- (1:n) * (H == tmp) k &lt;- min(k[k &gt; 0]) IsKnot[k] &lt;- 1 res2 &lt;- logcondens::LocalMLE(x, w, IsKnot, phi, prec) phi_new &lt;- res2$phi L &lt;- res2$L conv_new &lt;- res2$conv H &lt;- res2$H while ((max(conv_new) &gt; prec * max(abs(conv_new)))) { JJ &lt;- (1:n) * (conv_new &gt; 0) JJ &lt;- JJ[JJ &gt; 0] if (length(JJ) == 1 &amp;&amp; conv[JJ] == conv_new[JJ]){ # inserted print(&#39;break&#39;) break } tmp &lt;- conv[JJ]/(conv[JJ] - conv_new[JJ]) lambda &lt;- min(tmp) KK &lt;- (1:length(JJ)) * (tmp == lambda) KK &lt;- KK[KK &gt; 0] IsKnot[JJ[KK]] &lt;- 0 phi &lt;- (1 - lambda) * phi + lambda * phi_new conv &lt;- pmin(c(logcondens::LocalConvexity(x, phi), 0)) res3 &lt;- logcondens::LocalMLE(x, w, IsKnot, phi, prec) phi_new &lt;- res3$phi L &lt;- res3$L conv_new &lt;- res3$conv H &lt;- res3$H } phi &lt;- phi_new conv &lt;- conv_new if (sum(IsKnot != IsKnot_old) == 0) { break } if (print == TRUE) { print(paste(&quot;iter1 = &quot;, iter1 - 1, &quot; / L = &quot;, round(L, 4), &quot; / max(H) = &quot;, round(max(H), 4), &quot; / #knots = &quot;, sum(IsKnot), sep = &quot;&quot;)) } } Fhat &lt;- logcondens::LocalF(x, phi) res &lt;- list(xn = xn, x = x, w = w, phi = as.vector(phi), IsKnot = IsKnot, L = L, Fhat = as.vector(Fhat), H = as.vector(H), n = length(xn), m = n, knots = x[IsKnot == 1], mode = x[phi == max(phi)], sig = sig) return(res) } #&#39; calculating the surrogate loglikelihood Q #&#39; calc_surr_logcondens = function(X, g, resi, theta, alpha, idx, weight, lambda_alpha, lambda_theta){ K = length(g) N = sum(unlist(weight)) P = pi_k(X, alpha) # TT by K matrix p = dim(X)[2] TT = dim(X)[1] total = 0 for (k in 1:K){ for (t in 1:TT){ idx_tk = idx[[t]][,k] resi_tk = resi[[t]][idx_tk,k] weight_tk = weight[[t]][idx_tk,k] if (length(weight_tk) &gt; 0) { suppressWarnings({tmp = weight_tk * logcondens::evaluateLogConDens(resi_tk, g[[k]])[,2]}) total = total + sum(tmp[!is.infinite(tmp)]) + sum(weight_tk[!is.infinite(tmp)]) * log(P[t,k]) } } } Q = total/N - lambda_alpha * sum(abs(alpha[,2:(p+1)])) - lambda_theta * sum(abs(unlist(theta))) return(Q) } #Y_bin = Y_tr #X = X_tr #bin_mass = bin_mass_tr iteration = function(Y_bin, X, bin_mass, initial, lambda_alpha, lambda_theta, iter_eta = 1e-6, max_iter = 30, maxdev, r_bar){ K = length(initial$g_init) p = dim(X)[2] TT = dim(X)[1] idx_old = initial$idx_init resp_old = initial$resp_init weight_old = initial$weight_init resi_old = initial$resi_init alpha_old = initial$alpha_init theta0_old = initial$theta0_init theta_old = initial$theta_init g_old = initial$g_init Q = initial$Q Q_every = initial$Q_every # iteration for (i in seq(max_iter)) { ## E-step: update responsibilities Estep = E_step_logcondens(X, bin_mass, resi_old, alpha_old, g_old, r_bar) idx_new = Estep$idx resp_new = Estep$resp weight_new = Estep$weight # Q_every = append(Q_every, calc_surr_logcondens(X, g_old, resi_old, theta_old, alpha_old, idx_new, weight_new, lambda_alpha, lambda_theta)) print(&#39;passed Estep&#39;) ## M-step: update estimates of (alpha,theta,g) ### Mstep_alpha alpha_new = Mstep_alpha(X, weight_new, idx_new, lambda_alpha) # Q_every = append(Q_every, calc_surr_logcondens(X, g_old, resi_old, theta_old, alpha_new, idx_new, weight_new, lambda_alpha, lambda_theta)) print(&#39;passed alpha&#39;) ### Mstep_theta M_theta = Mstep_theta_logcondens(Y_bin, X, weight_new, resi_old, g_old, idx_old, theta0_old, theta_old, lambda_theta, maxdev) theta0_new = M_theta$theta0 theta_new = M_theta$theta print(&#39;passed theta&#39;) ### Mstep_shift theta0_new = Mstep_shift(Y_bin, X, weight_new, idx_new, theta_new) resi_new = calc_resi(Y_bin, X, theta0_new, theta_new) # Q_every = append(Q_every, calc_surr_logcondens(X, g_old, resi_new, theta_new, alpha_new, idx_new, weight_new, lambda_alpha, lambda_theta)) # save(resi_new, weight_new, idx_new, file = file.path(&#39;.&#39;, paste0(i, &#39;.Rdata&#39;))) # print(paste0(i, &#39;th input saved&#39;)) ### Mstep_g g_new = Mstep_g_logcondens(resi_new, weight_new, idx_new) Q_every = append(Q_every, calc_surr_logcondens(X, g_new, resi_new, theta_new, alpha_new, idx_new, weight_new, lambda_alpha, lambda_theta)) print(&#39;passed g&#39;) ### loglikelihood Q = append(Q, Q_every[length(Q_every)]) ## termination criteria print(i) inc = (Q[i+1]-Q[i])/abs(Q[i]) if (inc &lt; 0) { idx_new = idx_old resp_new = resp_old resi_new = resi_old alpha_new = alpha_old theta0_new = theta0_old theta_new = theta_old weight_new = weight_old g_new = g_old print(&quot;The loglikelihood decreased in the last iteration. Will return the previous parameters&quot;) break; } else if (inc &lt;= iter_eta | i==max_iter){ break; } else { idx_old = idx_new resi_old = resi_new alpha_old = alpha_new theta0_old = theta0_new theta_old = theta_new weight_old = weight_new g_old = g_new } } print(i) return(list(idx_new = idx_new, resp_new = resp_new, weight_new = weight_new, resi_new = resi_new, alpha_new = alpha_new, theta0_new = theta0_new, theta_new = theta_new, g_new = g_new, lambda_alpha = lambda_alpha, lambda_theta = lambda_theta, Q = Q, Q_every = Q_every)) } #&#39; Updating responsibility #&#39; E_step_logcondens = function(X, bin_mass, resi, alpha, g, r_bar){ K = length(g) TT = dim(X)[1] p = dim(X)[2] # initial clusters likeli = list() resp = list() weight = list() idx = list() for (t in 1:TT){ likeli[[t]] = matrix(0, nrow = nrow(resi[[t]]), ncol = K) idx[[t]] = matrix(F, nrow = nrow(resi[[t]]), ncol = K) for (k in 1:K){ suppressWarnings({likeli[[t]][,k] = logcondens::evaluateLogConDens(resi[[t]][,k], g[[k]])[,3] * pi_k(X, alpha)[t,k]}) } resp[[t]] = likeli[[t]]/rowSums(likeli[[t]]) idx[[t]] = resp[[t]] &gt; r_bar weight[[t]] = resp[[t]] * bin_mass[[t]] } return(list(resp = resp, weight = weight, idx = idx )) } #&#39; updating alpha #&#39; Mstep_alpha = function(X, weight, idx, lambda_alpha){ TT = dim(X)[1] K = dim(idx[[1]])[2] lambda_max = lambda_alpha * 100 lambdas = exp(seq(from = log(lambda_max), to = log(lambda_alpha), length = 30)) weight.sum = matrix(0, nrow = TT, ncol = K) for (t in 1:TT){ for (k in 1:K){ idx_tk = idx[[t]][,k] weight.sum[t,k] = sum(weight[[t]][idx_tk,k]) } } fit = glmnet::glmnet(x = X, y = weight.sum, lambda = lambdas, family = &quot;multinomial&quot;, intercept = TRUE) coefs = glmnet::coef.glmnet(fit, s = lambda_alpha) alpha = t(as.matrix(do.call(cbind, coefs))) return(alpha) # (p+1) by K matrix } #&#39; Updating theta #&#39; Mstep_theta_logcondens = function(Y_bin, X, weight, resi, g, idx_old, theta0, theta, lambda_theta, maxdev){ K = length(g) theta0_new = list() theta_new = list() for (k in 1:K){ tmp = LP_logcondens(Y_bin, X, weight, resi, g[[k]], idx_old, theta0[[k]], theta[[k]], lambda_theta, k, maxdev) theta0_new[[k]] = tmp$theta0_k theta_new[[k]] = tmp$theta_k } return(list(&#39;theta0&#39; = theta0_new , &#39;theta&#39; = theta_new )) } #&#39; Updating theta for each k (switching i and j) #&#39; LP_logcondens = function(Y_bin, X, weight, resi, g_k, idx, theta0_k, theta_k, lambda_theta, k, maxdev){ TT = length(Y_bin) N = sum(unlist(weight)) p = dim(X)[2] resi_k = c() w_k = c() idx_k = c() Y_idx = c() X_idx = c() n_to_skip = c() for (t in 1:TT){ idx_tk = idx[[t]][,k] nt = sum(idx_tk) if (nt != 0) { idx_k = c(idx_k, idx_tk) resi_k = c(resi_k, resi[[t]][idx_tk,k]) w_k = c(w_k, weight[[t]][idx_tk,k]) Y_idx = c(Y_idx, Y_bin[[t]][idx_tk,]) X_t = matrix(rep(X[t,], nt), byrow = T, ncol = p) X_idx = rbind(X_idx, X_t) } else { n_to_skip = c(n_to_skip, t) } } n = length(Y_idx) # the number of points in C_n x_m = g_k$x[as.logical(g_k$IsKnot)] phi_m = g_k$phi[as.logical(g_k$IsKnot)] J = length(x_m) - 1 # the number of affine functions beta = rep(0, J) b = rep(0, J) for (j in 1:J){ b[j] = (phi_m[j+1] - phi_m[j])/(x_m[j+1] - x_m[j]) beta[j] = b[j] * x_m[j] - phi_m[j] } L = min(resi_k) U = max(resi_k) const_mat = c() const_vec = c() # epigraph part for (j in 1:J){ tmp = cbind(Matrix::Diagonal(n), b[j], b[j]*X_idx) const_mat = rbind(const_mat, cbind(tmp, -tmp)) const_vec = c(const_vec, b[j]*Y_idx-beta[j]) } # feasibility part if (length(n_to_skip) == 0) { TT_new = TT X_new = X } else { TT_new = TT - length(n_to_skip) X_new = X[-n_to_skip,] } tmp = cbind(matrix(0, nrow = TT_new, ncol = n), matrix(1, nrow = TT_new, ncol = 1), X_new) const_mat = rbind(const_mat, cbind(tmp, -tmp), cbind(-tmp, tmp)) tmp = rep(0, 2*TT_new) count = 1 for (t in 1:TT){ idx_tk = idx[[t]][,k] if (sum(idx_tk) &gt; 0){ tmp[count] = min(Y_bin[[t]][idx_tk])- L tmp[TT_new + count] = U - max(Y_bin[[t]][idx_tk]) count = count + 1 } } const_vec = c(const_vec, tmp) # print(dim(const_mat)) # print(format(object.size(const_mat), &quot;Mb&quot;)) obj_coef = c(w_k, 0, rep(-N*lambda_theta, p), -w_k, 0, rep(-N*lambda_theta, p)) const_dir = rep(&quot;&lt;=&quot;, J*n + 2*TT_new) if (!is.null(maxdev)) { tmp = cbind(matrix(0, nrow = TT_new, ncol = n+1), X_new) const_mat = rbind(const_mat, cbind(tmp, -tmp), cbind(-tmp, tmp)) const_vec = c(const_vec, rep(maxdev, 2*TT_new)) const_dir = c(const_dir, rep(&quot;&lt;=&quot;, 2*TT_new)) } # solving LP lp_res = Rsymphony::Rsymphony_solve_LP(obj = obj_coef, mat = const_mat, dir = const_dir, rhs = const_vec, max = T) if (lp_res$status != 0) { print(&quot;No solution has been stored by Rsymphony. Change the LP solver to lpSolve&quot;) lp_res = lpSolve::lp(obj = obj_coef, const.mat = const_mat, const.dir = const_dir, const.rhs = const_vec, direction = &quot;max&quot;) } theta0_k = lp_res$solution[n+1] - lp_res$solution[2*n+p+2] theta_k = lp_res$solution[(n+2):(n+p+1)] - lp_res$solution[(2*n+p+3):(2*(n+p+1))] return(list(theta0_k = theta0_k, theta_k = theta_k)) #theta } L1_diff = function(true, est){ diff1 = sum(abs(true - est)) diff2 = sum(abs(true[,1] - est[,2]) + abs(true[,2] - est[,1])) return(min(diff1, diff2)) } isIncreasing = function(v){ n = length(v) for (i in 2:n){ if (v[i] &lt; v[i-1]) { return(F) } } return(T) } weightedHist = function(g_k, B = 30){ minY = min(g_k$xn) maxY = max(g_k$xn) bin = seq(from=minY, to=maxY, length= B + 1) binnedY = findInterval(g_k$xn, bin, rightmost.closed = T) binnedY = factor(binnedY, levels = 1:B) w = tapply(g_k$w, binnedY , sum) w[is.na(w)] = 0 w = w * B /(sum(w) *(maxY-minY)) mid = rep(0, B) for (i in 1:B){mid[i] = (bin[i+1] + bin[i])/2} return(list(mid = mid, w = w)) } weighted_quantile = function(x, w, thr = 0.05){ mat = cbind(x, w) mat = mat[order(x),] thr = thr * sum(w) tot = 0 i = 1 while (i &lt; length(w) &amp; tot &lt; thr){ tot = tot + w[i] i = i + 1 } return(mat[i,1]) } eval_LCD = function(LCDmix, Y_new, X_new, biomass_new, trim_thr = 0.05){ g = LCDmix$g_new theta = LCDmix$theta_new theta0 = LCDmix$theta0_new alpha = LCDmix$alpha_new resp = LCDmix$resp_new lambda_alpha = LCDmix$lambda_alpha lambda_theta = LCDmix$lambda_theta K = length(g) N = sum(unlist(biomass_new)) P = pi_k(X_new, alpha) # TT by K matrix p = dim(X_new)[2] TT = dim(X_new)[1] w_tt = unlist(biomass_new) loglike = c() for (t in 1:TT){ loglike_tk = rep(0, length(Y_new[[t]])) for (k in 1:K){ resi_tk = Y_new[[t]] - c(theta0[[k]] + sum(X_new[t,] * theta[[k]])) suppressWarnings({tmp = logcondens::evaluateLogConDens(resi_tk, g[[k]])[,3] * P[t,k]}) loglike_tk = loglike_tk + tmp } loglike = c(loglike, log(loglike_tk)) } prop = mean(is.infinite(loglike)) q = weighted_quantile(loglike, w_tt, trim_thr) trimmed = loglike[loglike &gt; q] w_trimmed = w_tt[loglike &gt; q] trimmed_logl = sum(trimmed * w_trimmed) / sum(w_trimmed) - lambda_alpha * sum(abs(alpha[,2:(p+1)])) - lambda_theta * sum(abs(unlist(theta))) return(list(loglike = loglike, w_tt = w_tt, prop = prop, trimmed = trimmed, w_trimmed = w_trimmed, trimmed_logl = trimmed_logl)) } CV_LCD_parallel = function(Y_bin, X, bin_mass, K, lambda_alpha_range = c(1e-8, 1e-1), lambda_theta_range = c(1e-8, 1e-1), cv_gridsize = 5, nrep_flowmix = 1, max_iter = 30, iter_eta = 1e-3, maxdev = NULL, r_bar = 1e-3, nfold = 5, blocksize = 20, cv_rep = 5, trim_thr = 0.05, save_folder = &#39;./result&#39;){ alpha_lambdas = sort(flowmix::logspace(min = lambda_alpha_range[1], max = lambda_alpha_range[2], length = cv_gridsize), decreasing = F) theta_lambdas = sort(flowmix::logspace(min = lambda_theta_range[1], max = lambda_theta_range[2], length = cv_gridsize), decreasing = F) print(alpha_lambdas) print(theta_lambdas) folds = flowmix::make_cv_folds(ylist = Y_bin, nfold = 5, blocksize = 5) iimat = c() for (a in 1:cv_gridsize){ for (b in 1:cv_gridsize){ for (c in 1:cv_rep){ for (d in 1:nfold){ iimat = rbind(iimat, cbind(alpha_lambdas[a], theta_lambdas[b], c, d))}}}} print(iimat) cl = parallel::makeCluster(parallel::detectCores(logical = FALSE) - 1) parallel::clusterExport(cl, c(&quot;main&quot;, &quot;binning&quot;, &quot;initialization&quot;, &quot;iteration&quot;, &quot;calc_resi&quot;, &quot;pi_k&quot;, &quot;Mstep_alpha&quot;, &quot;Mstep_theta_logcondens&quot;, &quot;LP_logcondens&quot;, &quot;Mstep_shift&quot;, &quot;Mstep_g_logcondens&quot;, &quot;calc_surr_logcondens&quot;, &quot;modified_logcondens&quot;, &quot;E_step_logcondens&quot;, &quot;weighted_quantile&quot;, &quot;eval_LCD&quot;, &quot;Y_bin&quot;, &quot;X&quot;, &quot;bin_mass&quot;, &quot;K&quot;, &quot;nrep_flowmix&quot;, &quot;max_iter&quot;, &quot;iter_eta&quot;, &quot;maxdev&quot;, &quot;r_bar&quot;, &quot;trim_thr&quot;, &quot;iimat&quot;, &quot;folds&quot;), envir = environment()) logs = parallel::parLapply(cl, 1:nrow(iimat), function(ii){ ialpha = iimat[ii, 1] itheta = iimat[ii, 2] irep = iimat[ii, 3] ifold = iimat[ii, 4] log_msg = paste(&quot;lambda_alpha = &quot;, ialpha, &quot; lambda_theta = &quot;, itheta, &quot; with &quot;, irep, &quot;th rep on &quot;, ifold, &quot;th fold started\\n&quot;) Y_tr = Y_bin[-folds[[ifold]]] bin_mass_tr = bin_mass[-folds[[ifold]]] X_tr= X[-folds[[ifold]], ] set.seed(irep) out_log = capture.output({res_ii = tryCatch({ main(Y_tr, X_tr, bin_mass_tr, binned = T, B = 0, K, ialpha, itheta, nrep_flowmix, max_iter, iter_eta, maxdev, r_bar)}, error = function(e) { message(paste0(&quot;Error for lambda_alpha = &quot;, ialpha, &quot; lambda_theta = &quot;, itheta, &quot; with &quot;, irep, &quot;th rep on &quot;, ifold, &quot;th fold.&quot;)) return(NA)})}) log_msg = paste0(log_msg, paste0(out_log, collapse = &quot;&quot;)) if (is.list(res_ii)){ Y_test = Y_bin[folds[[ifold]]] bin_mass_test = bin_mass[folds[[ifold]]] X_test= X[folds[[ifold]], ] eval_ii = eval_LCD(res_ii$iter, Y_test, X_test, bin_mass_test, trim_thr = trim_thr) prop_CV = eval_ii$prop trimmed_CV = eval_ii$trimmed_logl log_msg = paste0(log_msg, &quot;Saved results for lambda_alpha = &quot;, ialpha, &quot; lambda_theta = &quot;, itheta, &quot; with &quot;, irep, &quot;th rep on &quot;, ifold, &quot;th fold.\\n&quot;) } else { prop_CV = NA trimmed_CV = NA log_msg = paste0(log_msg, &quot;Error for lambda_alpha = &quot;, ialpha, &quot; lambda_theta = &quot;, itheta, &quot; with &quot;, irep, &quot;th rep on &quot;, ifold, &quot;th fold!\\n&quot;) } save_path &lt;- file.path(save_folder, paste0(ialpha, &#39;-&#39;, itheta, &#39;-&#39;, irep, &#39;-&#39;, ifold, &#39;-mac.Rdata&#39;)) save(prop_CV, trimmed_CV, file = save_path) return(log_msg) }) num_NA = sapply(logs, function(x){ if (substring(x, nchar(x)-1, nchar(x)-1) == &#39;!&#39;) { return(1) } else {return(0)} }) print(num_NA) print(paste0(&quot;Total &quot;, length(num_NA), &quot; calculations, &quot;, sum(num_NA), &quot; got NA results(&quot;, round(100 *mean(num_NA), 2), &quot;%).&quot; )) parallel::stopCluster(cl) return(logs) } CV_summary = function(lambda_alpha_range = c(1e-8, 1e-1), lambda_theta_range = c(1e-8, 1e-1), cv_gridsize = 5, cv_rep = 5, nfold = 5, save_folder = &#39;./result&#39;){ alpha_lambdas = sort(flowmix::logspace(min = lambda_alpha_range[1], max = lambda_alpha_range[2], length = cv_gridsize), decreasing = F) theta_lambdas = sort(flowmix::logspace(min = lambda_theta_range[1], max = lambda_theta_range[2], length = cv_gridsize), decreasing = F) CVmat = c() cgs = cv_gridsize for (a in 1:cgs){ for (b in 1:cgs){ for (irep in 1:cv_rep){ for (ifold in 1:nfold){ ialpha = alpha_lambdas[a] itheta = theta_lambdas[b] file_name = paste0(save_folder, paste0(ialpha, &#39;-&#39;, itheta, &#39;-&#39;, irep, &#39;-&#39;, ifold, &#39;-mac.Rdata&#39;)) load(file_name) CVmat = rbind(CVmat, cbind(ialpha, itheta, irep, ifold, prop_CV, trimmed_CV))}}}} max_NA_prop = max(CVmat[,5], na.rm = T) reduced_mat = c() size = cv_rep * nfold for (i in 1:(cgs**2)){ tmp = CVmat[((i-1)*size+1):(i*size), ] if (length(table(tmp[,1])) &gt; 1 | length(table(tmp[,2])) &gt; 1){ print(&#39;error&#39;) } tmp_CV = max(tapply(tmp[,6], factor(tmp[,3]), mean, na.rm = T)) reduced_mat = rbind(reduced_mat, cbind(tmp[1,1], tmp[1,2], tmp_CV)) } colnames(reduced_mat)[1:2] = c(&quot;ialpha&quot;, &quot;itheta&quot;) opt_lambdas = reduced_mat[which.max(reduced_mat[,3]),1:2] return(list(CVmat = CVmat, reduced_mat = reduced_mat, opt_lambdas = opt_lambdas, max_NA_prop = max_NA_prop)) } generate_data_1d_pseudoreal &lt;- function(bin = FALSE, seed=NULL, datadir=&quot;~/repos/cruisedat/export&quot;, nt = 200, beta_par = 0.5, p = 3, ## Number of total covariates dat.gridsize = 30, noisetype = c(&quot;gaussian&quot;, &quot;heavytail&quot;, &quot;skewed&quot;), df = NULL, skew_alpha = NULL, gap = 3, TT = 100, no_changepoint = FALSE, gradual_change = FALSE){ ## Setup and basic checks assertthat::assert_that(nt %% 5 ==0) ntlist = c(rep(0.8 * nt, TT/2), rep(nt, TT/2)) numclust = 2 stopifnot(p &gt;= 3) noisetype = match.arg(noisetype) if (noisetype == &quot;heavytail&quot;){ assertthat::assert_that(!is.null(df)) } if (noisetype == &quot;skewed&quot;){ assertthat::assert_that(!is.null(skew_alpha)) } ## Generate covariate ## datadir = &quot;~/repos/cruisedat/export&quot; load(&quot;MGL1704-hourly-only-binned.Rdata&quot;) par = X[, &quot;par&quot;] par = par[!is.na(par)] par = stats::ksmooth(x = 1:length(par), y = par, bandwidth = 5, x.points = 1:length(par))$y ## sst = X[,&quot;sst&quot;] ## In the future, maybe add another covariate if(!is.null(seed)) set.seed(seed) Xrest = do.call(cbind, lapply(1:(p-2), function(ii) stats::rnorm(TT)) ) X = cbind(scale(par[1:TT]), c(rep(0, TT/2), rep(1, TT/2)), Xrest)## p-2 columns colnames(X) = c(&quot;par&quot;, &quot;cp&quot;, paste0(&quot;noise&quot;, 1:(p-2))) ## Beta coefficients beta = matrix(0, ncol = numclust, nrow = p+1) beta[0+1,1] = 0 beta[1+1,1] = beta_par beta[0+1,2] = gap beta[1+1,2] = -beta_par colnames(beta) = paste0(&quot;clust&quot;, 1:numclust) rownames(beta) = c(&quot;intercept&quot;, &quot;par&quot;, &quot;cp&quot;, paste0(&quot;noise&quot;, 1:(p-2))) ## alpha coefficients alpha = matrix(0, ncol = numclust, nrow = p+1) alpha[0+1, 2] = -10 alpha[2+1, 2] = 10 + log(1/4) colnames(alpha) = paste0(&quot;clust&quot;, 1:numclust) rownames(alpha) = c(&quot;intercept&quot;, &quot;par&quot;, &quot;cp&quot;, paste0(&quot;noise&quot;, 1:(p-2))) ## Generate means and probabilities if(no_changepoint){ ## Only designed for TT=100 beta = beta[-3,] alpha = alpha[-3,] alpha[1, 2] = -1.4 X = X[,-2] } if(gradual_change){ X[,2] = c(1:(TT/2), (TT/2):1) alpha[0+1, 2] = -1.4 ## a = 0.05 a = 0.05 * 100 / TT alpha[2+1, 2] = a } mnmat = cbind(1, X) %*% beta prob = exp(cbind(1,X) %*% alpha) prob = prob/rowSums(prob) ## Samples |nt| memberships out of (1:numclust) according to the probs in prob. ## Data is a probabilistic mixture from these two means, over time. ylist = lapply(1:TT, function(tt){ draws = sample(1:numclust, size = ntlist[tt], replace = TRUE, ## prob = c(prob[[tt]], 1-prob[[tt]])) prob = c(prob[tt,1], prob[tt,2])) mns = mnmat[tt,] means = mns[draws] ## Add noise to obtain data points. if(noisetype == &quot;gaussian&quot;){ noise = stats::rnorm(ntlist[tt], 0, 1) } else if (noisetype == &quot;heavytail&quot;){ assertthat::assert_that(df &gt;= 3) variance = df / (df - 2) noise = stats::rt(ntlist[tt], df = df) / sqrt(variance) } else if (noisetype == &quot;skewed&quot;){ omega = sqrt(1/(1 - 2 * (1/pi) * skew_alpha^2 / (1 + skew_alpha^2))) mn_shift = omega * skew_alpha * (1 / sqrt(1+skew_alpha^2)) * sqrt(2/pi) noise = sn::rsn(ntlist[tt], xi = 0, omega = omega, alpha = skew_alpha) - mn_shift } else { stop(&quot;not written yet&quot;) } datapoints = means + noise cbind(datapoints) }) ## To bin or not! if(!bin){ countslist = NULL } else { ## stop(&quot;Binning doesn&#39;t work yet! make_grid and bin_many_cytograms aren&#39;t written for 1d data yet.&quot;) ## 1. Make grid dat.grid = flowmix::make_grid(ylist, gridsize = dat.gridsize) ## 2. Bin with just counts mc.cores = 4 obj = flowmix::bin_many_cytograms(ylist, dat.grid, mc.cores = mc.cores, verbose = TRUE) ybin_list = obj$ybin_list counts_list = obj$counts_list sparsecounts_list = obj$sparsecounts_list ## new ## 4. (NOT USED) Also obtain all the binned midpoints, in a (d^3 x 3) matrix. ## ybin_all = make_ybin(counts = NULL, midpoints = make_midpoints(dat.grid)) ybin_all = obj$ybin_all ## new ## Assign binned data to static names |ylist| and |countslist|. ylist = ybin_list countslist = counts_list } ## Other things about the true generating model sigma = array(NA, dim=c(2,1,1)) sigma[1,1,1] = sigma[2,1,1] = 1 mn = array(NA,dim=c(100,1,2)) mn[,1,] = mnmat numclust=2 return(list(ylist = ylist, X = X, countslist = countslist, ## The true generating model: mnmat = mnmat, prob = prob, mn = mn, numclust = numclust, alpha = alpha, beta = beta, sigma = sigma)) } "],["conclude.html", "4 Conclusion", " 4 Conclusion When you are done defining the package, it remains to convert the Roxygen to documentation. litr::document() # &lt;-- use instead of devtools::document() ## ℹ Updating LCDmix documentation ## Writing &#39;NAMESPACE&#39; ## ℹ Loading LCDmix ## Writing &#39;NAMESPACE&#39; ## Writing &#39;LCDmix-package.Rd&#39; ## Writing &#39;pi_k.Rd&#39; ## Writing &#39;calc_surr_logcondens.Rd&#39; ## Writing &#39;E_step_logcondens.Rd&#39; ## Writing &#39;Mstep_alpha.Rd&#39; ## Writing &#39;Mstep_theta_logcondens.Rd&#39; ## Writing &#39;LP_logcondens.Rd&#39; ## Writing &#39;pipe.Rd&#39; You can also add some extra things to your package here if you like, such as a README, some vignettes, a pkgdown site, etc. See here for an example of how to do this with litr. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
